# ğŸš€ Quickstart - Toonkit

GuÃ­a rÃ¡pida de 5 minutos para empezar con **toonkit**.

---

## InstalaciÃ³n

```bash
pip install toonkit
```

---

## 1. ConversiÃ³n BÃ¡sica

```python
from toonkit import encode, decode

# Tu data JSON
data = {
    "users": [
        {"id": 1, "name": "Alice", "role": "admin"},
        {"id": 2, "name": "Bob", "role": "user"}
    ]
}

# JSON â†’ TOON (ahorra tokens)
toon = encode(data)
print(toon)
# Output:
# users[2]{id,name,role}:
#   1,Alice,admin
#   2,Bob,user

# TOON â†’ JSON (recupera los datos)
original = decode(toon)
assert original == data  # âœ… IdÃ©ntico
```

---

## 2. CLI RÃ¡pida

```bash
# Convertir archivo
toonkit convert data.json -o data.toon

# Benchmark (ver ahorro de tokens)
toonkit benchmark data.json -m gpt-4

# Validar round-trip
toonkit validate data.json
```

---

## 3. Benchmark en CÃ³digo

```python
from toonkit.benchmark import TokenBenchmark

benchmark = TokenBenchmark()
result = benchmark.compare(data, model="gpt-4")

print(f"Tokens ahorrados: {result.token_reduction_pct:.1f}%")
# Output: Tokens ahorrados: 42.0% ğŸš€
```

---

## 4. Caso de Uso Real: LLM Prompt

```python
from toonkit import encode
import openai

# Data grande de una API
api_data = fetch_products_from_db()  # 100 productos

# Convertir a TOON antes de enviar al LLM
toon_prompt = encode(api_data)

# Enviar a OpenAI (ahorra 40% de tokens = 40% menos costo)
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "Analiza estos productos:"},
        {"role": "user", "content": toon_prompt}
    ]
)

# El modelo entiende TOON perfectamente y responde
print(response.choices[0].message.content)
```

**Ahorro tÃ­pico**: 
- 1,000 tokens â†’ 600 tokens
- $0.03 â†’ $0.018 por request
- $30,000 â†’ $18,000 por millÃ³n de requests

---

## 5. ConfiguraciÃ³n Avanzada

```python
from toonkit import ToonConfig, ParserMode

config = ToonConfig(
    sort_keys=True,           # Orden canÃ³nico (para caching)
    mode=ParserMode.STRICT,   # ValidaciÃ³n estricta
    max_depth=10,             # LÃ­mite de anidamiento
    max_size_mb=50,           # LÃ­mite de tamaÃ±o
)

toon = encode(data, config)
```

---

## 6. Streaming para Datasets Grandes

```python
from toonkit import encode_streaming

# Encode lÃ­nea por lÃ­nea (ideal para >100MB)
for line in encode_streaming(massive_dataset):
    socket.send(line)  # EnvÃ­a progresivamente
```

---

## PrÃ³ximos Pasos

- ğŸ“– Lee el [README completo](README.md)
- ğŸ§ª Ejecuta los [ejemplos](examples/)
- ğŸ§ª Prueba tus propios datos
- ğŸ“Š Mide el ahorro con `toonkit benchmark`

---

**Â¿Preguntas?** Abre un issue en GitHub o lee la [documentaciÃ³n completa](README.md).

ğŸ‰ **Â¡Empieza a ahorrar tokens hoy!**

